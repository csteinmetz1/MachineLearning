{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "**Christian Steinmetz**\n",
    "\n",
    "Due on November 22nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "We set up an experimental framework to study various aspects of overfitting. \n",
    "The input space is $X = [−1, 1]$ with uniform input probability density, $P(x) =\\frac{1}{2}$.\n",
    "We consider the two models $\\mathcal{H}_2$ and $\\mathcal{H}_{10}$. \n",
    "The target function is a polynomial of degree $Q_f$, which we write as $f(x)=\\sum_{q=0}^{Q_f} a_q L_q(x)$, where $Lq(x)$ are the Legendre polynomials.\n",
    "We use the Legendre polynomials because they are a convenient orthogonal basis for the polynomials on $[−1, 1]$.\n",
    "The first two Legendre polynomials are $L_0(x) = 1$, $L_1(x) = x$.\n",
    "The higher order Legendre polynomials are defined by the recursion:\n",
    "\n",
    "\\begin{equation}\n",
    "    L_k(x) = \\frac{2k-1}{k} x L_{k-1}(x) - \\frac{k-1}{k} L_{k-2}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is $\\mathcal{D} = (x_1, y_1), ..., (x_N , y_N)$, where $y_n = f (x_n)+\\sigma \\epsilon_n$ and $\\epsilon_n$ are i.i.d. standard Normal random variables.\n",
    "\n",
    "For a single experiment, with specified values for $Q_f$, $N$, $\\sigma$, generate a random degree $Q_f$ target function by selecting coefficients $a_q$ independently from a standard Normal distribution, rescaling them so that $\\mathbb{E}_{a,x}[f^2]=1$. Generate a data set, selecting\n",
    "$x_1,...,x_N$ independently from $P(x)$ and $y_n = f (x_n)+\\sigma \\epsilon_n$. \n",
    "\n",
    "Let $g_2$ and $g_{10}$ be the best fit hypotheses to the data from $\\mathcal{H_2}$ and $\\mathcal{H}_{10}$, respectively, with respective out-of sample errors $E_{out}(g_2)$ and $E_{out}(g_{10})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legendre_poly(k, x):\n",
    "    if k == 0:\n",
    "        return 1\n",
    "    elif k == 1:\n",
    "        return x\n",
    "    else:\n",
    "        return ((2*k-1)/k) * x * legendre_poly(k-1, x) - ((k-1)/k) * legendre_poly(k-2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_a_q(Q_f=None):\n",
    "    \n",
    "    if not Q_f:\n",
    "        # deteremine the degree, Q_f\n",
    "        Q_f = np.random.randint(3, 10)\n",
    "    \n",
    "    # sample values for a_q\n",
    "    a_q = np.random.standard_normal(size=Q_f)\n",
    "    \n",
    "    #a_q[0] = 0 \n",
    "    # for simplicity we hold the first coef at 0\n",
    "    # which means that E(f(x)) = 0\n",
    "    \n",
    "    # normalize by finding the variance of f(x)\n",
    "    Ef2 = np.array([a**2/((2*q)+1) for q, a in enumerate(a_q)]).sum() #+ a_q[0]**2\n",
    "    \n",
    "    return a_q / np.sqrt(Ef2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a_q):\n",
    "    #return np.sum([a * legendre_poly(k, x) for k, a in enumerate(a_q)])\n",
    "    \n",
    "    ploy = legendre(len(a_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a_q):\n",
    "    return np.polynomial.legendre.legval(x, a_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(f, a_q, sigma, N):\n",
    "    X = (np.random.rand(N) * 2) - 1 # uniform over [-1, 1]\n",
    "    Y = np.polynomial.legendre.legval(X, a_q) + (sigma * np.random.standard_normal(size=N))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X    mean: 0.00, var: 0.33\n",
      "Y    mean: 0.84, var: 0.29\n"
     ]
    }
   ],
   "source": [
    "a_q = generate_a_q(Q_f=2)\n",
    "\n",
    "X, Y = generate_dataset(f, a_q, 0.0, 1000000)\n",
    "print(f\"X    mean: {X.mean():.2f}, var: {X.var():.2f}\")\n",
    "print(f\"Y    mean: {Y.mean():.2f}, var: {Y.var():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Why do we normalize $f$?** [Hint: how would you interpret σ?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based upon it's appearence in $y_n$, $\\sigma$ appears to be a scaling factor of the Gaussian noise term $\\sigma \\epsilon_n$, since $\\epsilon_n \\sim N(0, 1)$ . This scaling term has the effect of increasing the variance of the distribution of this noise term, since larger values of $\\sigma$ will result in greater spread in likely values of the noise. \n",
    "\n",
    "We know also that our polynomial coefficents, $a_q$, are drawn from a standard normal as well, and we also know that the second moment of a random variable $X$ is given by $\\mathbb{E}(X^2)$, which corresponds to its varience. By normalizing our coefficients, $a_q$, of the target function $f(x)$, so that $\\mathbb{E}_{x}[f^2]=1$, we enforce the output of our function to be a normal random variable ($\\mu=a_0$ and $\\sigma=\\text{Var}(f)$).\n",
    "\n",
    "From a higher level, we can see that $y_n$ is the sum of two standard normal random variables, where the term $\\sigma$ controls the relative influence of the gaussian noise term, $\\sigma\\epsilon_n$. When $\\sigma=1$ the output of $y_n$ is the direct sum of two standard normals, whereas when $\\sigma=0$ the output is just the output of $f$.\n",
    "\n",
    "In conclusion, we propose that the input, $X$, to our target function, $f$, can be modled as a uniform random variable with $P(x) = \\frac{1}{2}$. The output of is given as $y_n = f (x_n)+\\sigma \\epsilon_n$, and we notice is is the sum of our normalized function $f \\sim N(0,1)$ and the noise term $\\sigma \\epsilon_n \\sim N(0,\\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) How can we obtain $g_2$, $g_{10}$?** [Hint: pose the problem as linear regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best fit hypothesis we can use all of the datapoints in $D$ to fit a linear regression model within $\\mathcal{H}_2$ and $\\mathcal{H}_{10}$, where we attempt to minimize the in-sample error for each model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_2 = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_2 = H_2.fit_transform(X.reshape(-1,1))\n",
    "\n",
    "H_10 = preprocessing.PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_10 = H_10.fit_transform(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_2 = LinearRegression(fit_intercept=True)\n",
    "g_2.fit(X_2, Y)\n",
    "\n",
    "g_10 = LinearRegression()\n",
    "g_10.fit(X_10, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How can we compute $E_{out}$ analytically for a given $g_{10}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the target function, $f$, and our best fit hypothesis will also provide a function, we can measure the distance between these to calculate $E_{out}$. A perfect prediction of $f$ will mean that our best fit hypothesis matches the exact same target function exactly. \n",
    "\n",
    "More generally, for any predicted target function $f_n$, we can define the expected error as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{err} = \\int_{X \\times Y} \\mathcal{L}(f_n(x), y) \\rho(x,y) dxdy\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{L}$ is a loss function (ex: $\\mathcal{L}(\\hat{y}, y) = \\mathbb{E}[ (y - \\hat{y})^2 ]$)   and $\\rho(x,y)$ is the joint probabaility of $x$ and $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Vary $Q_f$, $N$, $\\sigma$ and for each combination of parameters, run a large number of experiments, each time computing $E_{out}(g_2)$ and $E_{out}(g_{10})$.**\n",
    "\n",
    "Averaging these out-of-sample errors gives estimates of the expected out-of-sample error for the given learning scenario ($Q_f$, $N$, $\\sigma$) using $\\mathcal{H_2}$ and $\\mathcal{H_{10}}$. \n",
    "\n",
    "Let \n",
    "\n",
    "\\begin{equation}\n",
    "    E_{out}(\\mathcal{H}_2) = \\text{average over experiments}(E_{out}(g_2)), \\\\\n",
    "    E_{out}(\\mathcal{H}_{10}) = \\text{average over experiments}(E_{out}(g_{10})).\n",
    "\\end{equation}\n",
    "\n",
    "Define the overfit measure $E_{out}(\\mathcal{H}_{10}) − E_{out}(\\mathcal{H}_2)$. When is the overfit measure significantly positive (i.e. overfitting is serious) as opposed to significantly\n",
    "negative? Try the choices $Q_f \\in \\{1, 2,..., 100\\}, N \\in \\{20, 25,..., 120\\}, \\sigma^2 \\in \\{0, 0.05, 0.1,..., 2\\}$.\n",
    "\n",
    "Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_models(Q_f, N, s):\n",
    "    a_q = generate_a_q(Q_f=Q_f)\n",
    "    X, Y = generate_dataset(f, a_q, s, N)\n",
    "    \n",
    "    H2 = preprocessing.PolynomialFeatures(degree=2, include_bias=True)\n",
    "    X2 = H2.fit_transform(X.reshape(-1,1))\n",
    "    g2 = LinearRegression(fit_intercept=True)\n",
    "    g2.fit(X2, Y)\n",
    "    \n",
    "    H10 = preprocessing.PolynomialFeatures(degree=10, include_bias=True)\n",
    "    X10 = H10.fit_transform(X.reshape(-1,1))\n",
    "    g10 = LinearRegression()\n",
    "    g10.fit(X10, Y)\n",
    "\n",
    "    # this is an approx. of E_out\n",
    "    x = np.linspace(-1,1,1000)\n",
    "    y = np.polynomial.legendre.legval(x, a_q)\n",
    "    y2_hat = g2.predict(H2.fit_transform(x.reshape(-1,1)))\n",
    "    y10_hat = g10.predict(H10.fit_transform(x.reshape(-1,1)))\n",
    "    \n",
    "    print(a_q[0])\n",
    "    print(y2_hat.mean())\n",
    "    print(y10_hat.mean())\n",
    "    print()\n",
    "    print(a_q[0]**2)\n",
    "    print(np.mean(y2_hat**2))\n",
    "    print(np.mean(y10_hat**2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    E_out_g2  = np.power(y - y2_hat,2).mean()\n",
    "    E_out_g10 = np.power(y - y10_hat,2).mean()\n",
    "    \n",
    "    return E_out_g2, E_out_g10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7336239731172762\n",
      "0.732268912905997\n",
      "0.7322689129059974\n",
      "\n",
      "0.538204133932378\n",
      "0.9996722489517377\n",
      "0.9996722489517369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.923258534876786e-29, 7.43938604675619e-29)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models(3, 1000000, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q_f': 1, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 0.0, 'E_out(H10)': 0.0, 'overfit': 0.0}\n",
      "{'Q_f': 1, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.10264516525441272, 'E_out(H10)': 858.1534219632698, 'overfit': 858.0507767980154}\n",
      "{'Q_f': 1, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 0.0, 'E_out(H10)': 0.0, 'overfit': 0.0}\n",
      "{'Q_f': 1, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.04968258687305303, 'E_out(H10)': 1.09889383733648, 'overfit': 1.049211250463427}\n",
      "{'Q_f': 1, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 0.0, 'E_out(H10)': 0.0, 'overfit': 0.0}\n",
      "{'Q_f': 1, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.03688631796707949, 'E_out(H10)': 0.3901382022321957, 'overfit': 0.3532518842651162}\n",
      "{'Q_f': 1, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 0.0, 'E_out(H10)': 0.0, 'overfit': 0.0}\n",
      "{'Q_f': 1, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.033450791077470864, 'E_out(H10)': 0.1739913777525378, 'overfit': 0.14054058667506694}\n",
      "{'Q_f': 1, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 0.0, 'E_out(H10)': 0.0, 'overfit': 0.0}\n",
      "{'Q_f': 1, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.02490827228918664, 'E_out(H10)': 0.12457589141983727, 'overfit': 0.09966761913065063}\n",
      "{'Q_f': 1, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 0.0, 'E_out(H10)': 0.0, 'overfit': 0.0}\n",
      "{'Q_f': 1, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.025638420674856804, 'E_out(H10)': 0.14556046696604324, 'overfit': 0.11992204629118644}\n",
      "{'Q_f': 2, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 4.7829657734569264e-32, 'E_out(H10)': 1.8837109940293286e-27, 'overfit': 1.883663164371594e-27}\n",
      "{'Q_f': 2, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.11570753088463744, 'E_out(H10)': 27904.885124431923, 'overfit': 27904.76941690104}\n",
      "{'Q_f': 2, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 5.233230838094374e-32, 'E_out(H10)': 5.469644579523612e-31, 'overfit': 4.946321495714174e-31}\n",
      "{'Q_f': 2, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.07134991961200834, 'E_out(H10)': 11.59206450695997, 'overfit': 11.520714587347962}\n",
      "{'Q_f': 2, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 6.8486985637286e-32, 'E_out(H10)': 6.586448950869555e-31, 'overfit': 5.901579094496695e-31}\n",
      "{'Q_f': 2, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.037693941243332925, 'E_out(H10)': 0.3575966645388367, 'overfit': 0.3199027232955038}\n",
      "{'Q_f': 2, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 1.1502850692441765e-31, 'E_out(H10)': 2.75510960831721e-31, 'overfit': 1.6048245390730334e-31}\n",
      "{'Q_f': 2, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.03188644276079029, 'E_out(H10)': 0.27563532677682206, 'overfit': 0.24374888401603179}\n",
      "{'Q_f': 2, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 2.6931074668842923e-32, 'E_out(H10)': 4.553085571203465e-31, 'overfit': 4.283774824515035e-31}\n",
      "{'Q_f': 2, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.04129286396079773, 'E_out(H10)': 0.32454259790808304, 'overfit': 0.2832497339472853}\n",
      "{'Q_f': 2, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 7.724485412885792e-32, 'E_out(H10)': 2.3118504550194883e-31, 'overfit': 1.539401913730909e-31}\n",
      "{'Q_f': 2, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.017917953487770526, 'E_out(H10)': 0.111230848490535, 'overfit': 0.09331289500276446}\n",
      "{'Q_f': 3, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 1.1582173126401065e-31, 'E_out(H10)': 3.465072697035672e-29, 'overfit': 3.453490523909271e-29}\n",
      "{'Q_f': 3, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.14165755198344043, 'E_out(H10)': 23211.01236405075, 'overfit': 23210.87070649877}\n",
      "{'Q_f': 3, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 7.841012120306317e-32, 'E_out(H10)': 1.7224231318938198e-30, 'overfit': 1.6440130106907567e-30}\n",
      "{'Q_f': 3, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.05883532259651443, 'E_out(H10)': 0.7607117944528251, 'overfit': 0.7018764718563106}\n",
      "{'Q_f': 3, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 1.0748975362201095e-31, 'E_out(H10)': 6.756945586822648e-31, 'overfit': 5.682048050602538e-31}\n",
      "{'Q_f': 3, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.03836516314816532, 'E_out(H10)': 0.24503579325424846, 'overfit': 0.20667063010608314}\n",
      "{'Q_f': 3, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 6.277110451885265e-32, 'E_out(H10)': 5.02821340464867e-31, 'overfit': 4.400502359460144e-31}\n",
      "{'Q_f': 3, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.0527122462697092, 'E_out(H10)': 0.3133896966175885, 'overfit': 0.26067745034787926}\n",
      "{'Q_f': 3, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 9.229921148234082e-32, 'E_out(H10)': 3.5194658975366486e-31, 'overfit': 2.5964737827132404e-31}\n",
      "{'Q_f': 3, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.03387261939825485, 'E_out(H10)': 0.71448629731148, 'overfit': 0.6806136779132251}\n",
      "{'Q_f': 3, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 1.2391095669779383e-31, 'E_out(H10)': 9.869066619287996e-31, 'overfit': 8.629957052310058e-31}\n",
      "{'Q_f': 3, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.028389709700456196, 'E_out(H10)': 0.16061612621162766, 'overfit': 0.13222641651117145}\n",
      "{'Q_f': 4, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 0.27038771929174216, 'E_out(H10)': 3.9898802561131187e-25, 'overfit': -0.27038771929174216}\n",
      "{'Q_f': 4, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.5941014704608495, 'E_out(H10)': 1611465.2022761912, 'overfit': 1611464.6081747208}\n",
      "{'Q_f': 4, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 0.10605475474012165, 'E_out(H10)': 8.015784918303862e-31, 'overfit': -0.10605475474012165}\n",
      "{'Q_f': 4, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.1554252032282007, 'E_out(H10)': 2.2729278628707816, 'overfit': 2.117502659642581}\n",
      "{'Q_f': 4, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 0.09668685372247507, 'E_out(H10)': 9.659722754594805e-31, 'overfit': -0.09668685372247507}\n",
      "{'Q_f': 4, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.1686198560072978, 'E_out(H10)': 0.6235394895086923, 'overfit': 0.4549196335013945}\n",
      "{'Q_f': 4, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 0.132211721056754, 'E_out(H10)': 7.045751993240014e-31, 'overfit': -0.132211721056754}\n",
      "{'Q_f': 4, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.14509079531388225, 'E_out(H10)': 1.4499863127361405, 'overfit': 1.3048955174222583}\n",
      "{'Q_f': 4, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 0.17424745771639352, 'E_out(H10)': 6.0226072076782895e-31, 'overfit': -0.17424745771639352}\n",
      "{'Q_f': 4, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.1491919743310401, 'E_out(H10)': 0.16764392667281858, 'overfit': 0.018451952341778488}\n",
      "{'Q_f': 4, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 0.08853828044879793, 'E_out(H10)': 3.0905485709320916e-31, 'overfit': -0.08853828044879793}\n",
      "{'Q_f': 4, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.07213524725311074, 'E_out(H10)': 0.582658849282554, 'overfit': 0.5105236020294432}\n",
      "{'Q_f': 5, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 0.17948099053841612, 'E_out(H10)': 1.5437321549692802e-25, 'overfit': -0.17948099053841612}\n",
      "{'Q_f': 5, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.5020839508868006, 'E_out(H10)': 29.515852713206083, 'overfit': 29.013768762319284}\n",
      "{'Q_f': 5, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 0.20825252676314232, 'E_out(H10)': 1.943296882142687e-30, 'overfit': -0.20825252676314232}\n",
      "{'Q_f': 5, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.4098835407590905, 'E_out(H10)': 267.48686961290286, 'overfit': 267.07698607214377}\n",
      "{'Q_f': 5, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 0.2565265355523326, 'E_out(H10)': 1.2452452639956529e-30, 'overfit': -0.2565265355523326}\n",
      "{'Q_f': 5, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.2027998013522808, 'E_out(H10)': 0.6371351241025015, 'overfit': 0.4343353227502207}\n",
      "{'Q_f': 5, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 0.18013879348976355, 'E_out(H10)': 1.006212556652486e-30, 'overfit': -0.18013879348976355}\n",
      "{'Q_f': 5, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.27657589437165564, 'E_out(H10)': 0.42908555288529315, 'overfit': 0.1525096585136375}\n",
      "{'Q_f': 5, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 0.37956498476248557, 'E_out(H10)': 2.1488773409576828e-30, 'overfit': -0.37956498476248557}\n",
      "{'Q_f': 5, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.26450773509442815, 'E_out(H10)': 0.18946377524208774, 'overfit': -0.07504395985234041}\n",
      "{'Q_f': 5, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 0.1529067078136318, 'E_out(H10)': 1.1874042957119539e-30, 'overfit': -0.1529067078136318}\n",
      "{'Q_f': 5, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.30451681001237035, 'E_out(H10)': 0.13099637376853332, 'overfit': -0.17352043624383703}\n",
      "{'Q_f': 10, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 0.43796567719568547, 'E_out(H10)': 2.58375819425819e-24, 'overfit': -0.43796567719568547}\n",
      "{'Q_f': 10, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.7274679312784157, 'E_out(H10)': 492.6724209930092, 'overfit': 491.9449530617308}\n",
      "{'Q_f': 10, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 0.6637848224331784, 'E_out(H10)': 1.5878579046498097e-27, 'overfit': -0.6637848224331784}\n",
      "{'Q_f': 10, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.6970062155084891, 'E_out(H10)': 18.433630349203902, 'overfit': 17.736624133695415}\n",
      "{'Q_f': 10, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 0.36257593777800357, 'E_out(H10)': 6.586709127876164e-28, 'overfit': -0.36257593777800357}\n",
      "{'Q_f': 10, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.496295684045431, 'E_out(H10)': 0.46180373755898907, 'overfit': -0.03449194648644194}\n",
      "{'Q_f': 10, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 0.4579164534733716, 'E_out(H10)': 2.6553853288285443e-28, 'overfit': -0.4579164534733716}\n",
      "{'Q_f': 10, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.3107456562017344, 'E_out(H10)': 0.23046442081158086, 'overfit': -0.08028123539015353}\n",
      "{'Q_f': 10, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 0.30215376971593233, 'E_out(H10)': 1.1037719291016888e-28, 'overfit': -0.30215376971593233}\n",
      "{'Q_f': 10, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.4874617591761992, 'E_out(H10)': 0.19981873544047485, 'overfit': -0.28764302373572437}\n",
      "{'Q_f': 10, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 0.31996874386990537, 'E_out(H10)': 1.5471663345657873e-28, 'overfit': -0.31996874386990537}\n",
      "{'Q_f': 10, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.4745173845041724, 'E_out(H10)': 0.11093248326123965, 'overfit': -0.36358490124293275}\n",
      "{'Q_f': 20, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 0.5478350533608219, 'E_out(H10)': 135.0891548094702, 'overfit': 134.54131975610937}\n",
      "{'Q_f': 20, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.6106194151484907, 'E_out(H10)': 929.1024367645483, 'overfit': 928.4918173493999}\n",
      "{'Q_f': 20, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 0.4732145559788824, 'E_out(H10)': 2.939380066859338, 'overfit': 2.4661655108804554}\n",
      "{'Q_f': 20, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.5039860309813984, 'E_out(H10)': 12.7888788165837, 'overfit': 12.284892785602302}\n",
      "{'Q_f': 20, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 0.5612276084693926, 'E_out(H10)': 0.7764041761316179, 'overfit': 0.21517656766222537}\n",
      "{'Q_f': 20, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.6283043585731776, 'E_out(H10)': 0.5694599954319408, 'overfit': -0.058844363141236755}\n",
      "{'Q_f': 20, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 0.5473475674755927, 'E_out(H10)': 0.4983864883720206, 'overfit': -0.04896107910357206}\n",
      "{'Q_f': 20, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.5635582924985052, 'E_out(H10)': 0.6386135722707456, 'overfit': 0.07505527977224036}\n",
      "{'Q_f': 20, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 0.3933152695535339, 'E_out(H10)': 1.5517629780966775, 'overfit': 1.1584477085431435}\n",
      "{'Q_f': 20, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.5874606624229882, 'E_out(H10)': 0.83402550941345, 'overfit': 0.24656484699046177}\n",
      "{'Q_f': 20, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 0.5092942298325068, 'E_out(H10)': 0.15115360445670206, 'overfit': -0.35814062537580477}\n",
      "{'Q_f': 20, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.6169474907393837, 'E_out(H10)': 0.5317149237004495, 'overfit': -0.08523256703893423}\n",
      "{'Q_f': 50, 'N': 20, 'Sigma': 0.0, 'E_out(H2)': 0.6350912052810564, 'E_out(H10)': 11354441.986409822, 'overfit': 11354441.351318616}\n",
      "{'Q_f': 50, 'N': 20, 'Sigma': 1.0, 'E_out(H2)': 0.704943875573038, 'E_out(H10)': 55111.9076017964, 'overfit': 55111.20265792083}\n",
      "{'Q_f': 50, 'N': 40, 'Sigma': 0.0, 'E_out(H2)': 0.6480089979629636, 'E_out(H10)': 1.0009535826283287, 'overfit': 0.35294458466536505}\n",
      "{'Q_f': 50, 'N': 40, 'Sigma': 1.0, 'E_out(H2)': 0.7885683397475388, 'E_out(H10)': 636.1048794322252, 'overfit': 635.3163110924777}\n",
      "{'Q_f': 50, 'N': 60, 'Sigma': 0.0, 'E_out(H2)': 0.5818794414030972, 'E_out(H10)': 0.7009074921867914, 'overfit': 0.11902805078369416}\n",
      "{'Q_f': 50, 'N': 60, 'Sigma': 1.0, 'E_out(H2)': 0.726825483826252, 'E_out(H10)': 2.6285863893960464, 'overfit': 1.9017609055697944}\n",
      "{'Q_f': 50, 'N': 80, 'Sigma': 0.0, 'E_out(H2)': 0.6236542786577044, 'E_out(H10)': 0.6835889553964117, 'overfit': 0.05993467673870734}\n",
      "{'Q_f': 50, 'N': 80, 'Sigma': 1.0, 'E_out(H2)': 0.6943549831868965, 'E_out(H10)': 4.736002562031581, 'overfit': 4.041647578844685}\n",
      "{'Q_f': 50, 'N': 100, 'Sigma': 0.0, 'E_out(H2)': 0.46760860612339944, 'E_out(H10)': 0.47837092259283437, 'overfit': 0.01076231646943493}\n",
      "{'Q_f': 50, 'N': 100, 'Sigma': 1.0, 'E_out(H2)': 0.6069672725083307, 'E_out(H10)': 0.7531008686273435, 'overfit': 0.14613359611901278}\n",
      "{'Q_f': 50, 'N': 120, 'Sigma': 0.0, 'E_out(H2)': 0.611313696275144, 'E_out(H10)': 0.43583518755606265, 'overfit': -0.17547850871908133}\n",
      "{'Q_f': 50, 'N': 120, 'Sigma': 1.0, 'E_out(H2)': 0.6632243473808288, 'E_out(H10)': 0.5804720916467883, 'overfit': -0.08275225573404055}\n"
     ]
    }
   ],
   "source": [
    "Q_f = np.array([1, 2, 3, 4, 5, 10, 20, 50])\n",
    "N   = np.arange(20,121, 20)\n",
    "Sig = [0.0, 1.0]#np.arange(0,2.05,0.05)\n",
    "T   = np.arange(10)\n",
    "\n",
    "n_iters = len(Q_f) * len(N) * len(Sig) * len(T)\n",
    "iters = 0\n",
    "\n",
    "logs = []\n",
    "\n",
    "for q_f in Q_f:\n",
    "    for n in N:\n",
    "        for s in Sig:\n",
    "            Eh2, Eh10 = [], []\n",
    "            for t in T:\n",
    "                Eg2, Eg10 = fit_models(q_f, n, s)\n",
    "                Eh2.append(Eg2)\n",
    "                Eh10.append(Eg10)\n",
    "                iters += 1\n",
    "                \n",
    "                # print some progress\n",
    "                sys.stdout.write(f\"{iters:4d}/{n_iters:4d} - {(iters/n_iters)*100:2.2f}%\\r\")\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            Eh2 = np.array(Eh2).mean()\n",
    "            Eh10 = np.array(Eh10).mean()\n",
    "            \n",
    "            results = {\n",
    "                \"Q_f\" : q_f,\n",
    "                \"N\" : n,\n",
    "                \"Sigma\" : s,\n",
    "                \"E_out(H2)\" : Eh2,\n",
    "                \"E_out(H10)\" : Eh10,\n",
    "                \"overfit\" : Eh10 - Eh2}\n",
    "            \n",
    "            logs.append(results)\n",
    "            \n",
    "for entry in logs:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Why do we take the average over many experiments?** Use the variance to select an acceptable number of experiments to average over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
