{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "**Christian Steinmetz**\n",
    "\n",
    "Due on November 22nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "We set up an experimental framework to study various aspects of overfitting. \n",
    "The input space is $X = [−1, 1]$ with uniform input probability density, $P(x) =\\frac{1}{2}$.\n",
    "We consider the two models $\\mathcal{H}_2$ and $\\mathcal{H}_{10}$. \n",
    "The target function is a polynomial of degree $Q_f$, which we write as $f(x)=\\sum_{q=0}^{Q_f} a_q L_q(x)$, where $Lq(x)$ are the Legendre polynomials.\n",
    "We use the Legendre polynomials because they are a convenient orthogonal basis for the polynomials on $[−1, 1]$.\n",
    "The first two Legendre polynomials are $L_0(x) = 1$, $L_1(x) = x$.\n",
    "The higher order Legendre polynomials are defined by the recursion:\n",
    "\n",
    "\\begin{equation}\n",
    "    L_k(x) = \\frac{2k-1}{k} x L_{k-1}(x) - \\frac{k-1}{k} L_{k-2}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is $\\mathcal{D} = (x_1, y_1), ..., (x_N , y_N)$, where $y_n = f (x_n)+\\sigma \\epsilon_n$ and $\\epsilon_n$ are i.i.d. standard Normal random variables.\n",
    "\n",
    "For a single experiment, with specified values for $Q_f$, $N$, $\\sigma$, generate a random degree $Q_f$ target function by selecting coefficients $a_q$ independently from a standard Normal distribution, rescaling them so that $\\mathbb{E}_{a,x}[f^2]=1$. Generate a data set, selecting\n",
    "$x_1,...,x_N$ independently from $P(x)$ and $y_n = f (x_n)+\\sigma \\epsilon_n$. \n",
    "\n",
    "Let $g_2$ and $g_{10}$ be the best fit hypotheses to the data from $\\mathcal{H_2}$ and $\\mathcal{H}_{10}$, respectively, with respective out-of sample errors $E_{out}(g_2)$ and $E_{out}(g_{10})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legendre_poly(k, x):\n",
    "    if k == 0:\n",
    "        return 1\n",
    "    elif k == 1:\n",
    "        return x\n",
    "    else:\n",
    "        return ((2*k-1)/k) * x * legendre_poly(k-1, x) - ((k-1)/k) * legendre_poly(k-2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_a_q():\n",
    "        \n",
    "    # deteremine the degree, Q_f\n",
    "    Q_f = np.random.randint(3, 10)\n",
    "    \n",
    "    # sample values for a_q\n",
    "    a_q = np.random.standard_normal(size=Q_f)\n",
    "    \n",
    "    # normalize for E(f^2) = 1\n",
    "    E = np.mean([f(-1, a_q)**2, f(1, a_q)**2])\n",
    "    \n",
    "    return a_q/E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a_q):\n",
    "    return np.sum([a * legendre_poly(k, x) for k, a in enumerate(a_q)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(f, a_q, sigma, N):\n",
    "    X = np.random.choice([-1, 1], size=N)\n",
    "    Y = f(X, a_q) + (sigma * np.random.standard_normal(size=N))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_q = generate_a_q()\n",
    "X, Y = generate_dataset(f, a_q, 0.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Why do we normalize $f$? [Hint: how would you interpret σ?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) How can we obtain $g_2$, $g_{10}$? [Hint: pose the problem as linear regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) How can we compute $E_{out}$ analytically for a given $g_{10}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Vary $Q_f$, $N$, $\\sigma$ and for each combination of parameters, run a large number of experiments, each time computing $E_{out}(g_2)$ and $E_{out}(g_{10})$. \n",
    "Averaging these out-of-sample errors gives estimates of the expected out-of-sample error for the given learning scenario ($Q_f$, $N$, $\\sigma$) using $\\mathcal{H_2}$ and $\\mathcal{H_{10}}$. \n",
    "\n",
    "Let \n",
    "\n",
    "\\begin{equation}\n",
    "    E_{out}(\\mathcal{H}_2) = \\text{average over experiments}(E_{out}(g_2)), \\\\\n",
    "    E_{out}(\\mathcal{H}_{10}) = \\text{average over experiments}(E_{out}(g_{10})).\n",
    "\\end{equation}\n",
    "\n",
    "Define the overfit measure $E_{out}(H\\mathcal{H}_{10}) − E_{out}(\\mathcal{H}_2)$. When is the overfit measure significantly positive (i.e. overfitting is serious) as opposed to significantly\n",
    "negative? Try the choices $Q_f \\in \\{1, 2,..., 100\\}, N \\in \\{20, 25,..., 120\\}, \\sigma^2 \\in \\{0, 0.05, 0.1,..., 2\\}$.\n",
    "\n",
    "Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Why do we take the average over many experiments? Use the variance to select an acceptable number of experiments to average over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
